INFO 11-18 16:07:49 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-18 16:07:49 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-18 16:07:53 selector.py:135] Using Flash Attention backend.
INFO 11-18 16:07:54 model_runner.py:1072] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...
INFO 11-18 16:07:55 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-18 16:09:15 weight_utils.py:288] No model.safetensors.index.json found in remote.
INFO 11-18 16:09:16 model_runner.py:1077] Loading model weights took 2.8797 GB
INFO 11-18 16:09:17 worker.py:232] Memory profiling results: total_gpu_memory=23.68GiB initial_memory_usage=3.31GiB peak_torch_memory=4.28GiB memory_usage_post_profile=3.34GiB non_torch_memory=0.46GiB kv_cache_size=17.77GiB gpu_memory_utilization=0.95
INFO 11-18 16:09:18 gpu_executor.py:113] # GPU blocks: 41587, # CPU blocks: 9362
INFO 11-18 16:09:18 gpu_executor.py:117] Maximum concurrency for 4096 tokens per request: 162.45x
INFO 11-18 16:09:22 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-18 16:09:22 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-18 16:10:03 model_runner.py:1518] Graph capturing finished in 41 secs, took 0.20 GiB
18
18
18
18
3
3
3
3
70000
120000
195000
40000
180
180
180
180
20
140
140
20
64
64
64
64
260
260
260
260
120
120
120
140
60
45
135
315} \text{ miles
460
460
460
460
366
366
366
366
694
694
694
694
12
1
12
12
18
18
18
18
60
60
60
60
125
125
96
125
310
230
230
310
57500
57500
57500
57500
7
7
7
7
6
6
6
8
15
15
\frac{47}{3}
15
23
8
14
2
7
7
7
7
8
8
8
8
26
26
26
26
2
2
2
2
243
243
243
243
16
16
16
16
25
25
25
25
104
104
104
104
109
109
109
109
80
80
80
80
35
35
35
35
70
70
70
70
23
23
23
23
9
9
9
9
75
75
75
75
5
0
1
0
10
10
10
10
18
18
18
18
8
8
8
8
1200
1000
600
200
26
26
26
26
48
47
48
48
-160
20
30
20
104
104
104
104
163
163
163
163
800
800
800
800
8
8
8
8
30
30
30
30
294
294
294
294
5
5
5
\frac{4d}{15} \text{ hours}} \]

If we had a specific distance, we could substitute it in to get a numerical answer. For example, if the distance \(d\) were 30 miles, then the time it takes Tom to get back would be:
\[ \frac{4 \times 30}{15} = 8 \text{ hours
15
15
15
15
40
40
40
40
40
33
40
40
14
14
14
14
3
3
3
3
83
83
83
83
57.00
54.00
57
57
187
187
187
187
17
17
17
17
1430
1430
1430
1430
2500
2500
2500
2500
1596
1596
1596
1596
300
300
300
300
36
36
36
36
48
48
48
48
595
595
595
595
36
36
36
36
60
60
60
60
7425
7425
7425
7425
60
60
60
60
221
221
221
221
255
255
255
255
88
88
88
88
60
7.5
7.5
60
5
5
5
5
100
100
100
100
6
6
6.00
6
70
70
70
70
10
10
10
10
17
17
17
17
623
623
623
623
600
600
600
600
15
15
15
15
44
44
44
44
22
22
22
22
798.60
798.60
780
780
8000
8000
8000
8000
18
18
18
18
225
225
225
225
28
28
28
28
4
4
4
4
36.36
44
36.36
36} \text{ seconds
348
348
348
348
40
40
40
40
3
3
3
3
12
12
12
12
5
5
5
5
40
40
40
40
ACCURACY: 87.0
